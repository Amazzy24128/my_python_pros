from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from markdownify import markdownify as md
import os
import time
import signal
from urllib.parse import urlparse, urljoin
from selenium.common.exceptions import TimeoutException
from shutil import which

SAVE_FILE = "/home/amz/æ–‡æ¡£/my_python_pros/cj_get/cangjie_docs_markdown/all_docs.md"
visited_urls = set()

def setup_driver():
    """é…ç½® Chrome æµè§ˆå™¨"""
    chrome_options = Options()
    chrome_options.add_argument('--headless=new')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
    
    driver_path = which("chromedriver")
    if driver_path:
        print(f"ğŸ”§ ä½¿ç”¨: {driver_path}")
        service = Service(driver_path)
        service.log_path = os.devnull
        driver = webdriver.Chrome(service=service, options=chrome_options)
    else:
        driver = webdriver.Chrome(options=chrome_options)
    
    driver.set_page_load_timeout(30)
    driver.implicitly_wait(3)
    return driver

def get_page_content(driver, url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½: {url}")
        driver.get(url)
        
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        time.sleep(2)  # ç­‰å¾… JavaScript æ‰§è¡Œ
        
        return driver.page_source
    except TimeoutException:
        print(f"â±ï¸ åŠ è½½è¶…æ—¶")
        return driver.page_source
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None

def extract_doc_links(driver, current_url):
    """æå–æ–‡æ¡£é“¾æ¥"""
    links = set()
    base_domain = "docs.cangjie-lang.cn"
    
    try:
        # è·å–æ‰€æœ‰é“¾æ¥
        link_elements = driver.find_elements(By.TAG_NAME, "a")
        
        for element in link_elements:
            try:
                href = element.get_attribute('href')
                if not href:
                    continue
                
                # åªä¿ç•™åŒåŸŸåä¸‹çš„ HTML æ–‡æ¡£é“¾æ¥
                if base_domain in href and href.endswith('.html'):
                    if href not in visited_urls:
                        links.add(href)
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                elif href.endswith('.html') and not href.startswith('http'):
                    full_url = urljoin(current_url, href)
                    if base_domain in full_url and full_url not in visited_urls:
                        links.add(full_url)
            except:
                continue
        
        # ä½¿ç”¨ BeautifulSoup è¡¥å……æå–
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            
            if href.endswith('.html'):
                if href.startswith('http') and base_domain in href:
                    if href not in visited_urls:
                        links.add(href)
                elif not href.startswith('http'):
                    full_url = urljoin(current_url, href)
                    if base_domain in full_url and full_url not in visited_urls:
                        links.add(full_url)
        
        print(f"   âœ… æå–åˆ° {len(links)} ä¸ªæ–°é“¾æ¥")
        
    except Exception as e:
        print(f"âš ï¸ æå–é“¾æ¥å¤±è´¥: {e}")
    
    return list(links)

def html_to_markdown(html, url):
    """å°† HTML è½¬æ¢ä¸º Markdown"""
    soup = BeautifulSoup(html, "html.parser")
    
    # ç§»é™¤æ— ç”¨æ ‡ç­¾
    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'iframe', 'noscript', 'svg']):
        tag.decompose()
    
    # ç§»é™¤å¯¼èˆªå…ƒç´ 
    for selector in ['.sidebar', '.menu', '.navigation', '.header', '.footer', '.toctree-wrapper']:
        for element in soup.select(selector):
            element.decompose()
    
    # æŸ¥æ‰¾ä¸»è¦å†…å®¹
    main_content = None
    selectors = [
        '[role="main"]',
        'main',
        'article',
        '.document',
        '.content',
        '.body',
        '.rst-content',
        '#main-content'
    ]
    
    for selector in selectors:
        main_content = soup.select_one(selector)
        if main_content:
            print(f"   âœ“ æ‰¾åˆ°å†…å®¹: {selector}")
            break
    
    if not main_content:
        main_content = soup.body if soup.body else soup
        print("   âš  ä½¿ç”¨ body")
    
    # è½¬æ¢ä¸º Markdown
    markdown_content = md(
        str(main_content),
        heading_style="ATX",
        bullets="-",
        code_language="cangjie",
        strip=['img', 'svg']
    )
    
    # æ¸…ç†å¤šä½™ç©ºè¡Œ
    lines = []
    prev_empty = False
    for line in markdown_content.split('\n'):
        is_empty = not line.strip()
        if is_empty and prev_empty:
            continue
        lines.append(line)
        prev_empty = is_empty
    
    markdown_content = '\n'.join(lines).strip()
    
    # æ·»åŠ æ ‡é¢˜å’Œæ¥æº
    title = soup.find('title')
    if title:
        title_text = title.get_text().strip()
        markdown_content = f"# {title_text}\n\n> æ¥æº: {url}\n\n{markdown_content}"
    
    return markdown_content

def save_markdown_to_file(markdown_content):
    """å°† Markdown å†…å®¹è¿½åŠ åˆ°å•ä¸€æ–‡ä»¶"""
    os.makedirs(os.path.dirname(SAVE_FILE), exist_ok=True)
    with open(SAVE_FILE, "a", encoding="utf-8") as f:
        f.write(markdown_content + "\n\n")
    print(f"âœ… å·²è¿½åŠ å†…å®¹åˆ° {SAVE_FILE}")

def crawl_page(driver, url):
    """çˆ¬å–å•ä¸ªé¡µé¢"""
    if url in visited_urls:
        return []
    
    visited_urls.add(url)
    
    html = get_page_content(driver, url)
    if not html:
        return []
    
    markdown_content = html_to_markdown(html, url)
    if len(markdown_content.strip()) > 100:  # åªä¿å­˜æœ‰æ•ˆå†…å®¹
        save_markdown_to_file(markdown_content)
    
    return extract_doc_links(driver, url)

def safe_quit_driver(driver):
    """å®‰å…¨å…³é—­æµè§ˆå™¨"""
    try:
        driver.quit()
    except:
        try:
            if hasattr(driver.service, 'process') and driver.service.process:
                os.kill(driver.service.process.pid, signal.SIGKILL)
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸš€ ä»“é¢‰ç¼–ç¨‹è¯­è¨€æ–‡æ¡£çˆ¬è™« (æ‰€æœ‰å†…å®¹è¿½åŠ åˆ°ä¸€ä¸ªæ–‡ä»¶)")
    print("=" * 70)
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {SAVE_FILE}\n")
    
    driver = None
    try:
        driver = setup_driver()
        
        # ç›´æ¥ä»æ–‡æ¡£ç«™ç‚¹å¼€å§‹çˆ¬å–
        start_urls = [
            "https://docs.cangjie-lang.cn/docs/1.0.3/user_manual/source_zh_cn/first_understanding/hello_world.html",
            # å¯ä»¥æ·»åŠ æ›´å¤šå…¥å£
            "https://docs.cangjie-lang.cn/docs/1.0.3/user_manual/source_zh_cn/index.html",
        ]
        
        queue = start_urls.copy()
        max_pages = 500  # å¢åŠ çˆ¬å–æ•°é‡
        
        print(f"ğŸ¯ èµ·å§‹é¡µé¢æ•°: {len(start_urls)}")
        print(f"ğŸ“Š æœ€å¤§çˆ¬å–æ•°: {max_pages}\n")
        
        while queue and len(visited_urls) < max_pages:
            current_url = queue.pop(0)
            
            print(f"\n{'='*70}")
            print(f"ğŸ“Š è¿›åº¦: {len(visited_urls)}/{max_pages}")
            
            new_links = crawl_page(driver, current_url)
            
            for link in new_links:
                if link not in visited_urls and link not in queue:
                    queue.append(link)
            
            print(f"   é˜Ÿåˆ—å‰©ä½™: {len(queue)} ä¸ª")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        print(f"\n{'='*70}")
        print(f"âœ¨ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š å…±çˆ¬å– {len(visited_urls)} ä¸ªé¡µé¢")
        print(f"ğŸ“‚ ä¿å­˜ä½ç½®: {SAVE_FILE}")
        print(f"{'='*70}")
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œå·²çˆ¬å– {len(visited_urls)} ä¸ªé¡µé¢")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if driver:
            safe_quit_driver(driver)
            print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")

if __name__ == "__main__":
    main()